model_paths:
  reward_model: 'results\reward\checkpoint-4221'
  reward_tokenizer: 'results\reward\checkpoint-4221'
  sft_model: 'lvwerra/gpt2-imdb'
  output_model: './results/warp_model'

training_params:
  iterations: 2
  training_steps: 100
  rl_runs: 2
  ema_update_rate: 0.01
  interpolation_factor: 0.5
  kl_coefficient: 0.5
  batch_size: 64
  learning_rate: 1e-5

dataset:
  num_positive_samples: 100
  num_negative_samples: 100
  max_length: 512
  max_prompts: 128
  test_max_prompts: 100
  min_tokens: 5
  max_tokens: 18

reward:
  output_dir: './results/reward'
  max_length: 512
  num_train_epochs: 3
  per_device_train_batch_size: 16

device: 'cuda'  
max_length: 25
