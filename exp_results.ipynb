{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, DistilBertForSequenceClassification, DistilBertTokenizer,\n",
    "import torch\n",
    "from evaluate import evaluate\n",
    "from warp_training import train_warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем эксперименты с изменением T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_model = GPT2LMHeadModel.from_pretrained('lvwerra/gpt2-imdb').to(device)\n",
    "reward_model = DistilBertForSequenceClassification.from_pretrained('results/reward/checkpoint-4221').to(device)\n",
    "reward_tokenizer = DistilBertTokenizer.from_pretrained('results/reward/checkpoint-4221')\n",
    "sft_model_tokenizer = GPT2Tokenizer.from_pretrained('lvwerra/gpt2-imdb')\n",
    "sft_model_tokenizer.pad_token = sft_model_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 0 of 200:   0%|          | 0/100 [00:00<?, ?it/s]c:\\ProgramData\\anaconda3\\envs\\loop_tf\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:544: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "c:\\Users\\konstantin\\Desktop\\alignment\\warp_training.py:104: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha = 1) (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\python_arg_parser.cpp:1581.)\n",
      "  ema_param.data.mul_(1 - mu).add_(mu, param.data)\n",
      "iteration: 0 of 200: 100%|██████████| 100/100 [04:08<00:00,  2.48s/it]\n",
      "iteration: 1 of 200: 100%|██████████| 100/100 [06:03<00:00,  3.64s/it]\n",
      "iteration: 2 of 200: 100%|██████████| 100/100 [06:21<00:00,  3.81s/it]\n",
      "iteration: 3 of 200: 100%|██████████| 100/100 [07:25<00:00,  4.46s/it]\n",
      "iteration: 4 of 200:  43%|████▎     | 43/100 [02:37<03:33,  3.74s/it]"
     ]
    }
   ],
   "source": [
    "reward_warp = {}\n",
    "kl_div_dict = {}\n",
    "for T in [100, 150, 200]:\n",
    "    # Обучение модели с заданными параметрами\n",
    "    config['training_params']['iterations'] = T\n",
    "    warp_model = train_warp(config)\n",
    "    \n",
    "    # Оценка моделей\n",
    "    warp_rewards, sft_rewards, kl_div = evaluate(\n",
    "        sft_model,\n",
    "        warp_model,\n",
    "        sft_model_tokenizer,\n",
    "        reward_model,\n",
    "        reward_tokenizer,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    # Сохранение результатов\n",
    "    reward_warp[T] = warp_rewards\n",
    "    kl_div_dict[T] = kl_div\n",
    "\n",
    "# Вывод результатов\n",
    "print('Reward Warp:', reward_warp)\n",
    "print('KL Divergence Dict:', kl_div_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По графику видим, что при увеличении итераций reward и KL увеличивается, т.к. мы напрямую оптимизируем нашу модель под ревард, а KL будет расти, потому что мы отдаляемся от изначального распределения, хотя мы и пытаемся оставлять ее на месте.\n",
    "Хочется подвести итоги по результатам экспериментов в ходе реализации алгоритма, о которых я не успел подготовить наглядные примеры. При увеличении $mu$ в EMA наша модель будет все дальше отходить от нашей изначальной модели в сторону policy, что увеличит KL и reward и мы берем ее достаточно маленькой, чтобы не терять начальное распределение и не терять обобщающую способность модели.\n",
    "$lambda$ определяет насколько модель будет склонна к одной из двух (или больше) моделей, которые мы интерполируем, поэтому нельзя однозначно сказать тенденция к росту или падению метрик.\n",
    "Аналогично при увеличении $thetta$ мы все больше отдаляемся от изначальных параметров модели в процессе интерполяции, что увеличит KL. Можно думать об этом как шаге в градиентном спуске (мы идем от начальной модели к желаемой с lr = $thetta$).\n",
    "\n",
    "Не успел: попробовать scheduler для $thetta$, оценить зависимость качества моделей от батчей и количества подаваемых данных\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loop_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
